<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.80.0" />


<title>mixVarClust Statistical Documentation - bouba&#39;s notes</title>
<meta property="og:title" content="mixVarClust Statistical Documentation - bouba&#39;s notes">


  <link href='../../../../favicons.ico' rel='icon' type='image/x-icon'/>


  







<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-P6FER778X2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-P6FER778X2');
</script>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  

  <ul class="nav-links">
    
    <li><a href="../../../../">Home</a></li>
    
    <li><a href="../../../../about/">About</a></li>
    
  </ul>
</nav>

      </header>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">14 min read</span>
    

    <h1 class="article-title">mixVarClust Statistical Documentation</h1>

    
    <span class="article-date">2020-11-20</span>
    

    <div class="article-content">
      
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title></title>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>

<h1 id="introduction">Introduction</h1>
<p>mixVarClust is a clustering package implemented in R. This post explains the models and algorithms implemented in the package.</p>
<p>Clustering analysis in unsupervised learning consists of finding partitions with the same structures in unlabeled data. There are several techniques in machine learning designed to carry out clustering with unknown labels, such as k-means, autoencoders, mixture models, etc. The package <code>mixVarClust</code> groups data by executing mixture models and the EM algorithm.</p>
<p>In section 1, I present the general case of a finite mixture model. In the sub-sections, I define the model, explain how to estimate its parameters using the EM algorithm, and how to retrieve the clusters sought. In the section 2 and 3, I introduce the well known Gaussian mixture model, and the Multinomial mixture model. In the last section, I present the gaussian-multinomial mixture model.</p>
<h1 id="the-finite-mixture-model">The finite mixture model</h1>
<p>Let <span class="math inline"><em>X</em> ∈ ℝ<sup><em>d</em></sup></span> be a dataset composed by <span class="math inline"><em>n</em></span> observations, and <span class="math inline"><em>d</em> ≥ 1</span> dimensions. We assume that <span class="math inline"><em>X</em></span> is composed of <span class="math inline"><em>K</em> ∈ ℕ</span> different <em>unknown</em> clusters with the same structure, and each cluster has its own probability distribution parameterized by different set of parameters. A finite mixture model represents the probabiliy distribution of the observation <span class="math inline"><em>x</em><sub><em>i</em></sub></span> parameterized by <span class="math inline"><em>Θ</em></span>, formally :</p>
<p><br /><span class="math display">$$f_X(\mathbf{x_i}) = \sum_{k=1}^{K}p_kf_k(x_i|\theta_k)
\label{mixmod}$$</span><br /></p>
<p>where :</p>
<ul>
<li><p><span class="math inline"><em>x</em><sub><em>i</em></sub> = (<em>x</em><sub><em>i</em>, 1</sub>, ..., <em>x</em><sub><em>i</em>, <em>d</em></sub>) ∈ <em>X</em></span> is a single observation. <span class="math inline"><em>X</em></span> is <span class="math inline"><em>n</em> × <em>d</em></span> matrix or a data table.</p></li>
<li><p><span class="math inline"><em>p</em><sub><em>k</em></sub></span> is the proportion of each group, also called mixing proportion. It is the probability that an observation was generated by the <span class="math inline"><em>k</em></span>th group : <br /><span class="math display"><em>p</em><sub><em>k</em></sub> = <em>P</em><em>r</em><em>o</em><em>b</em>(<em>z</em><sub><em>i</em><em>k</em></sub> = 1),</span><br /> where <span class="math inline"><em>k</em> ∈ <em>K</em></span>, <span class="math inline">0 &lt; <em>p</em><sub><em>k</em></sub> &lt; 1</span> and <span class="math inline">∑<sub><em>k</em></sub><em>p</em><sub><em>k</em></sub> = 1</span>. <span class="math inline"><em>z</em><sub><em>i</em><em>k</em></sub></span> is a latent variable in the clustering context.</p></li>
<li><p><span class="math inline"><em>Θ</em> = (<em>p</em><sub>1</sub>, ..., <em>p</em><sub><em>K</em></sub>, <em>θ</em><sub>1</sub>, ..., <em>θ</em><sub><em>K</em></sub>)</span> is the set of parameters of the marginal distribution (or the full mixture).</p></li>
<li><p><span class="math inline"><em>f</em><sub><em>k</em></sub>(<em>x</em><sub><em>i</em></sub>|<em>θ</em><sub><em>k</em></sub>)</span> is the probability distribution of <span class="math inline"><em>x</em><sub><em>i</em></sub></span> parameterized by <span class="math inline"><em>θ</em><sub><em>k</em></sub></span> in the <span class="math inline"><em>k</em></span>th component of the mixture. <span class="math inline"><em>θ</em><sub><em>k</em></sub></span> is the set of parameters of the density or probability mass function of <span class="math inline"><em>f</em><sub><em>k</em></sub></span>.</p></li>
</ul>
<p>For instance, if the data <span class="math inline"><em>X</em></span> has only continuous variables, <span class="math inline"><em>f</em><sub><em>k</em></sub>(.|<em>θ</em><sub><em>k</em></sub>)</span> can be a multivariate or univariate Gaussian distribution (in this case <span class="math inline"><em>θ</em><sub><em>k</em></sub> = (<em>μ</em><sub><em>k</em></sub>, <em>Σ</em><sub><em>k</em></sub>)</span>) or a student distribution, etc. If the dataset is entirely composed of categorical variables, <span class="math inline"><em>f</em><sub><em>k</em></sub></span> is usually the multinomial distribution (when there are multiple categories) or the binomial distribution (when we have binary variables). For mixed variables, continuous and categorical, <span class="math inline"><em>f</em><sub><em>k</em></sub></span> is composed of multiple distributions suitable to each feature of the dataset in hand.</p>
<h2 id="estimation-of-a-finite-mixture-model-with-unknown-labels">Estimation of a finite mixture model with unknown labels</h2>
<p>Like many models in statistics, the maximum likelihood estimation is applied to estimate mixture models, but combined to the EM algorithm.</p>
<h3 id="maximum-likelihood-estimation-mle">Maximum likelihood estimation (MLE)</h3>
<p>The parameters of a mixture model with <em>known</em> labels is estimated by the maximum likelihood. But in an unsupervised setting, we can show that, it is not possible to use the MLE directly. The likelihood function of the mixture model with unknown labels is defined as : <br /><span class="math display">$$L\left(\Theta;x_{i}\right)=\prod_{i=1}^{n} f_{X}\left(x_{i} \mid \Theta\right)=\prod_{i=1}^{n} \sum_{k} p_kf_{k}\left(x_{i} \mid \theta_{k}\right)\\$$</span><br /> And the log-likelihood function, also called <em>incomplete log-likelihood</em>, because the labels are not observed, is:</p>
<p><br /><span class="math display">$$\begin{aligned}
\ell(\Theta; x_{i}) &amp;= \log L(\Theta ; x_{i}) \notag \\
\ell\left(\Theta; x_{i}\right)&amp;=\log \left[\prod_{i=1}^{N} \sum_{k=1}^{K} p_{k} f_{k}\left(x_{i} \mid \theta_{k}\right)\right] \notag \\
\ell\left(\Theta; x_{i}\right)&amp;=\sum_{i=1}^{N} \log \left(\sum_{k=1}^{K} p_{k} f_{k}\left(x_{i} \mid \theta_{k}\right)\right)
\label{incomplik}\end{aligned}$$</span><br /></p>
<p>To find the maximum likelihood estimates, we need to maximize <span class="math inline">ℓ(<em>Θ</em>;<em>x</em><sub><em>i</em></sub>)</span> with respect to <span class="math inline"><em>Θ</em></span>, which implies the derivation of the <span class="math inline">ℓ(<em>Θ</em>;<em>x</em><sub><em>i</em></sub>)</span> with respect to <span class="math inline"><em>p</em><sub><em>k</em></sub></span> and <span class="math inline"><em>θ</em><sub><em>k</em></sub></span>. But the parameters <span class="math inline">ℓ(<em>Θ</em>;<em>x</em><sub><em>i</em></sub>)</span> in equation <a href="#incomplik" data-reference-type="ref" data-reference="incomplik">[incomplik]</a> cannot be derived mathematically, it is impossible to develop the <strong>log</strong> of sum. In contrast, in a classification context, where labels of the data <span class="math inline"><strong>z</strong></span> are known, the log-likelihood, also called <em>complete likelihood</em>, is defined as : <br /><span class="math display">$$\ell_c(\Theta; \mathbf{x_i}, z_{i}) = \sum_i^n \sum_k^K \tilde{z}_{ik}\log \big[p_kf_k(\mathbf{x_i}|\theta_k)\big] \label{complik},$$</span><br /></p>
<p>However, the parameters in <span class="math inline">ℓ(<em>Θ</em>;<em>x</em><sub><em>i</em></sub>)</span> can be approximated using the EM algorithm. This algorithm makes use of the complete log-liklihood (Eq. <a href="#complik" data-reference-type="ref" data-reference="complik">[complik]</a>) to approximate the parameters of the incomplete log-likelihood.</p>
<h2 id="the-em-algorithm">The EM algorithm </h2>
<p>The <strong>Expected-maximization</strong> (EM) algorithm is an iterative method to find a local maximum of a likelihood function in the presence of latent variables. Its goal is to approximate the parameters that maximize the likelihood. The algorithm consists of alternating two steps called E-Step and M-Step (<strong>E</strong> stands for Expectation and <strong>M</strong> for maximization).</p>
<dl>
<dt> E-Step</dt>
<dd><p><br />
We estimate the incomplete log-likelihood <span class="math inline">ℓ(<em>Θ</em>;<em>x</em><sub><em>i</em></sub>)</span> by computing the <em>expectation</em> of the complete likelihood (of Eq. <a href="#complik" data-reference-type="ref" data-reference="complik">[complik]</a>). Let <span class="math inline"><em>q</em> ∈ ℕ</span> be the <span class="math inline"><em>q</em><sup><em>t</em><em>h</em></sup></span> iteration and the function <span class="math inline"><em>Q</em></span> be the expectation of <span class="math inline">ℓ<sub><em>c</em></sub></span>, formally we get :</p>
<p><br /><span class="math display">$$Q\left(\Theta, \Theta^{q}\right)=E\left[\ell_{c}\left(\Theta; x_{i}, z_i\right) \mid x_{i}, \Theta^{q}\right] =\sum_i^N \sum_k^K  E\left[\tilde{z}_{i k}|{\mathbf{x_i}}, \boldsymbol{\Theta}^{q}\right] \log\big[p_kf_k(x_i \mid \theta_k)\big],$$</span><br /> where:</p>
<p><br /><span class="math display">$$\begin{aligned}
E\left[\tilde{z}_{i k}|{\mathbf{x_i}}, \boldsymbol{\Theta}^{q}\right] &amp;=1 \times P\left(\tilde{z}_{i k}=1 \mid \mathbf{x}_{i}, \boldsymbol{\theta}^{q}\right)+0 \times P\left(\tilde{z}_{i k}=0 \mid \mathbf{x}_{i}, \boldsymbol{\theta}^{q}\right) \notag \\
&amp;=\frac{f_{\mid \tilde{z}_{i k}=1}\left(\mathbf{x}_{i}| \boldsymbol{\theta}^{q}\right) P\left(\tilde{z}_{ik}=1 \mid \boldsymbol{\theta}^{q}\right)}{f_X\left(\mathbf{x}_{i}| \boldsymbol{\Theta}^{q}\right)} \notag \\
&amp;=\frac{f_{k}\left(\mathbf{x}_{i}| \boldsymbol{\theta}^{q}\right) p_{k}^{q}}{f_{\mathbf{X}}\left(\mathbf{x}_{i}|\Theta^{q}\right)} \notag \\
&amp;=t_{k}^{q}\left(\mathbf{x}_{i}|\mathbf{\Theta}^q\right) \notag\end{aligned}$$</span><br /></p>
<p>To simplify notation let denote <span class="math inline"><em>t</em><sub><em>i</em><em>k</em></sub><sup><em>q</em></sup> = <em>t</em><sub><em>k</em></sub><sup><em>q</em></sup>(<strong>x</strong><sub><em>i</em></sub>|<strong>Θ</strong><sup><em>q</em></sup>)</span>, the expection of the complete log-likelihood becomes :</p>
<p><br /><span class="math display">$$Q\left(\Theta, \Theta^{q}\right)=\sum_i^N \sum_k^K \mathbf{t_{ik}^{q}}\log\big[p_kf_k(x_i\mid\theta_k)\big]
\label{expectation}$$</span><br /> <span class="math inline"><em>t</em><sub><em>i</em><em>k</em></sub><sup><em>q</em></sup></span> is called the maximum a posteriori (MAP), it is the probability that the observation <span class="math inline"><em>x</em><sub><em>i</em></sub></span> belongs to the <span class="math inline"><em>k</em></span>th cluster.</p>
<p>In practice, only the MAP is computed in the E-Step. At the <em>first</em> iteration (<span class="math inline"><em>q</em> = 0</span>) of the algorithm, <span class="math inline"><em>t</em><sub><em>i</em><em>k</em></sub><sup>0</sup></span> is computed using arbitrary initial parameters, <span class="math inline"><em>Θ</em><sup>0</sup></span>. In the second and following iterations, the MAP is computed by using the <span class="math inline"><em>Θ</em><sup><em>q</em></sup></span> paramters, found in the M-Step at the <span class="math inline"><em>q</em></span>th iteration.</p>
<p><br /><span class="math display">$$t_{k}^{q}\left(\mathbf{x}_{i} \mid \Theta^{q}\right)=\frac{p_{k}^{q} f_k\left(\mathbf{x}_{i} \mid \boldsymbol{\theta}_{k}^{q}\right)}{\sum_{\ell=1}^{K} p_{\ell}^{q} f_\ell\left(\mathbf{x}_{i} \mid \boldsymbol{\theta}_{\ell}^{q}\right)}$$</span><br /></p>
</dd>
<dt> M-Step</dt>
<dd><p><br />
In this step we maximise the expectation <span class="math inline"><em>Q</em></span> with respect to <span class="math inline"><em>Θ</em> = (<em>p</em><sub><em>k</em></sub>, <em>θ</em><sub><em>k</em></sub>)</span>. We determine the new estimates of the parameters <span class="math inline"><strong>Θ</strong><sup><em>q</em> + 1</sup></span> that maximize the <em>expected</em> complete log-likelihood (the Eq. <a href="#expectation" data-reference-type="ref" data-reference="expectation">[expectation]</a>):</p>
<p><br /><span class="math display">$$\boldsymbol{\Theta}^{q+1}=\underset{\boldsymbol{\Theta}}{\operatorname{argmax}} Q\left(\boldsymbol{\Theta}, \boldsymbol{\Theta}^{q}\right)
\label{max}$$</span><br /></p>
<p>Maximising <span class="math inline"><em>Q</em></span>, involves deriving the equation <a href="#expectation" data-reference-type="ref" data-reference="expectation">[expectation]</a> with respect to the parameters, and subject to <span class="math inline">∑<sub><em>k</em></sub><em>p</em><sub><em>k</em></sub> = 1</span>. Additional constraints on the objective function <span class="math inline"><em>Q</em></span> maybe necessary depending on the probability distribution <span class="math inline"><em>f</em><sub><em>k</em></sub></span>. By using the lagrangian, we can derive the estimates <span class="math inline"><em>p̂</em><sub><em>k</em></sub></span> and <span class="math inline"><em>θ̂</em><sub><em>k</em></sub></span>.</p>
</dd>
<dt>Termination</dt>
<dd><p><br />
Define a stopping rule to terminate the algorithm.</p>
</dd>
</dl>
<p>For each iteration of the algorithm, update the parameters : <br /><span class="math display"><em>Θ</em><sup><em>q</em></sup> = <em>Θ</em><sup><em>q</em> + 1</sup></span><br /></p>
<h3 id="stopping-rules-used-in-mixvarclust">Stopping rules used in mixVarClust</h3>
<p>There are several ways for stopping the EM algorithm. Two ways are implemented in <code>mixVarClust</code>. The first stopping rule is the convergence of the incomplete log-likelihood, which is computed by using the new found parameters in the M-Step. Formally, we evaluate: <br /><span class="math display">|ℓ(<em>x</em>, <em>Θ</em><sup><em>q</em> + 1</sup>) − ℓ(<em>x</em>, <em>Θ</em><sup><em>q</em></sup>)| &lt; <em>ϵ</em>.</span><br /></p>
<p>The second rule is an arbitrary pre-defined number of iterations of the algorithm. When the convergence of the log-likelihood takes too much time, it is better to use this second rule.</p>
<h3 id="initialization-strategy-in-mixvarclust">Initialization strategy in mixVarClust</h3>
<p>There are many ways to choose the initial parameters of the EM algorithm. In <code>mixVarClust</code>, I initialized randomly the labels of the data using a vector <span class="math inline"><em>z</em></span>, then I computed the estimates <span class="math inline"><em>Θ</em><sup>0</sup></span> and the incomplete log-likelihood (equation <a href="#incomplik" data-reference-type="ref" data-reference="incomplik">[incomplik]</a>). I repeated this process multiple times, then I chose as initial parameters, the parameters which gave the highest log-likelihood value.</p>
<h3 id="retrieving-the-found-clusters">Retrieving the found clusters</h3>
<p>The end goal of clustering is to find the labels of the data. Let <span class="math inline"><strong>z̃</strong> = {<strong>z̃</strong><sub>1</sub>, ..., <strong>z̃</strong><sub><em>n</em></sub>}</span>, with <span class="math inline"><strong>z̃</strong><sub><em>i</em></sub> = (<strong>z̃</strong><sub><em>i</em>1</sub>, ..., <strong>z̃</strong><sub><em>i</em><em>K</em></sub>)</span>, be the labels. If <span class="math inline"><em>x</em><sub><em>i</em></sub></span> is assigned to the <span class="math inline"><em>k</em><em>t</em><em>h</em></span> cluster, <span class="math inline"><strong>z̃</strong><sub><em>i</em><em>k</em></sub> = 1</span>, else <span class="math inline">0</span>.</p>
<p>There are two commonly used maximum likelihood approaches to attain this goal. The classification approach and mixture approach (for more details see <span class="citation" data-cites="rmixmod">(Biernacki et al. 2008)</span> and <span class="citation" data-cites="celeux_classification_1992">(Celeux and Govaert 1992)</span>). The classification approach consists of considering <span class="math inline"><em>z</em></span> as another unknown prameter and estimating it with <span class="math inline"><em>Θ</em></span>. In this case, the parameters are approximated using a variation of the EM algorithm called CEM algorithm. In <code>mixVarClust</code>, I am using the mixture approach. In this approch, the labels <span class="math inline"><em>z</em></span> are identified by using the <span class="math inline"><em>M</em><em>A</em><em>P</em></span> (Maximum A Posteriori) principle. Let <span class="math inline"><em>z</em><sub><em>i</em></sub></span> be the label of <span class="math inline"><em>x</em><sub><em>i</em></sub></span>, <span class="math inline">$\hat{\Theta} = (\hat{p_k},\hat{\theta_k})$</span> the estimate of <span class="math inline"><em>Θ</em> = (<em>p</em><sub><em>k</em></sub>, <em>θ</em><sub><em>k</em></sub>)</span> the MAP is as follows : <br /><span class="math display">$$\tilde{z}_{i k}=\left\{\begin{array}{l}
1 \text { if } k=\arg \max _{\ell=1 \ldots, K} t_{\ell}\left(\mathbf{x}_{i} \mid \hat{\Theta}\right) \\
0 \text { if not }
\end{array}\right.$$</span><br /> where : <br /><span class="math display">$$t_{k}\left(\mathbf{x}_{i} \mid \hat{\Theta}\right)=\frac{\hat{p}_{k}f_k\left(\mathbf{x}_{i} \mid \hat{\boldsymbol{\theta}}_{k}\right)}{\sum_{\ell=1}^{K} \hat{p}_{\ell} f_{\ell}\left(\mathbf{x}_{i} \mid \hat{\boldsymbol{\theta}}_{\ell}\right)},$$</span><br /> and <span class="math inline"><em>Θ̂</em></span> is the estimates found in the M-Step, retrieved at the last iteration of the EM algorithm. <span class="math inline"><em>t</em><sub><em>k</em></sub>(<strong>x</strong><sub><em>i</em></sub>∣<em>Θ̂</em>)</span> is the probability of having the observation <span class="math inline"><em>x</em><sub><em>i</em></sub></span> in the <span class="math inline"><em>k</em></span>th cluster given that <span class="math inline"><em>Θ</em></span> are the true parameters (it is the Bayes theorem). The MAP assumes that <span class="math inline"><em>x</em><sub><em>i</em></sub></span> belongs to the group <span class="math inline"><em>k</em></span> if <span class="math inline"><em>t</em><sub><em>i</em><em>k</em></sub></span> has the highest probability value. For instance, if <span class="math inline"><em>K</em> = 2</span>, <span class="math inline"><em>t</em><sub><em>i</em>1</sub> = 0.4</span> and <span class="math inline"><em>t</em><sub><em>i</em>2</sub> = 0.6</span>, then the observation <span class="math inline"><em>i</em></span> belongs to the cluster 2.</p>
<h1 id="the-gaussian-mixture-model-gmm">The Gaussian Mixture Model (GMM)</h1>
<p>The Gaussian mixture model supposes that the unknown groups in the dataset are each, distributed according to the Gaussian distribution. In this case, all the features in <span class="math inline"><em>X</em> = {<em>x</em><sub>1</sub>, ...<em>x</em><sub><em>d</em></sub>}</span> are continuous. The parameters of the model are <span class="math inline"><em>Θ</em> = (<em>p</em><sub><em>k</em></sub>, <em>θ</em><sub><em>k</em></sub>) where <em>θ</em><sub><em>k</em></sub> = (<em>μ</em><sub><em>k</em></sub>, <em>Σ</em><sub><em>k</em></sub>)</span>. The marginal distribution of an observation <span class="math inline"><em>x</em><sub><em>i</em></sub></span> is defined as:</p>
<p><br /><span class="math display">$$f_{\mathbf{X}}(\mathbf{x_i}) = \sum_{k=1}^{K}p_k f_k(\mathbf{x_i}|\mu_k, \Sigma_k)$$</span><br /> where the density of each component is the univariate or multivariate normal distribution :</p>
<p><br /><span class="math display">$$f_k(x_i|\mu_k, \Sigma_k) = \frac{1}{(2 \pi)^{p / 2}\left|\Sigma_{k}\right|^{1 / 2}} \exp\left\{-\frac{1}{2}\left(\mathbf{x_i}-\mu_{k}\right)^{t} \Sigma_{k}^{-1}\left(\mathbf{x_i}-\mu_{k}\right)\right\}
\label{eq:gauss}$$</span><br /></p>
<p>For the multivariate case (multiple features), <span class="math inline"><em>Σ</em><sub><em>k</em></sub></span> is a <span class="math inline"><em>d</em> × <em>d</em></span> matrix variance covariance and <span class="math inline"><em>μ</em><sub><em>k</em></sub></span> is a vector of length <span class="math inline"><em>d</em></span>. If <span class="math inline"><em>d</em> = 1</span>, i.e one feature, <span class="math inline"><em>f</em><sub><em>k</em></sub></span> is parametrized by 2 values, the mean and the variance.</p>
<h2 id="maximum-likelihood-estimation">Maximum likelihood estimation</h2>
<p>The incomplete log-likelihood of the GMM -derived from equation <a href="#incomplik" data-reference-type="ref" data-reference="incomplik">[incomplik]</a>- is : <br /><span class="math display">$$\ell\left(\Theta; x_{i}\right)=\sum_{i=1}^{N} \log \left(\sum_{k=1}^{K} p_{k}  \frac{1}{(2 \pi)^{p / 2}\left|\Sigma_{k}\right|^{1 / 2}} \exp \left\{-\frac{1}{2}\left(\mathbf{x_i}-\mu_{k}\right)^{t} \Sigma_{k}^{-1}\left(\mathbf{x_i}-\mu_{k}\right)\right\}
\right)$$</span><br /></p>
<h2 id="the-em-algorithm-for-gaussian-mixtures">The EM algorithm for Gaussian mixtures</h2>
<p>The algorithm starts with arbitrary estimates of the parameters for each component <span class="math inline"><em>Θ</em><sup>0</sup> = (<em>p̂</em><sub><em>k</em></sub>, <em>μ̂</em><sub><em>k</em></sub>, <em>Σ̂</em><sub><em>k</em></sub>)</span>, then alternates the computation of the E-Step and M-Step until convergence.</p>
<ul>
<li><p><strong>E-Step.</strong> Computes the MAP <span class="math inline"><em>t</em><sub><em>k</em></sub><sup><em>q</em></sup>(<strong>x</strong><sub><em>i</em></sub>∣<em>Θ</em><sup><em>q</em></sup>)</span>, using <span class="math inline"><em>Θ</em><sup>0</sup></span> at the first iteration of the algorithm, uses <span class="math inline"><em>Θ</em><sup><em>q</em></sup></span> for the next iterations.</p>
<p><br /><span class="math display">$$t_{ik}^{q} = \frac{\hat{p}_k f_k(\mathbf{x_i}|\hat{\mu}_{k}, \hat{\Sigma}_{k}) }{\sum_{\ell=1}^{K}\hat{p}_\ell f_\ell(\mathbf{x_i}|\hat{\mu}_{\ell}, \hat{\Sigma}_{\ell}) }$$</span><br /></p></li>
<li><p><strong>M-Step.</strong> Computes <span class="math inline"><em>Θ</em><sup><em>q</em> + 1</sup> = (<em>p̂</em><sub><em>k</em></sub>, <em>μ̂</em><sub><em>k</em></sub>, <em>Σ̂</em><sub><em>k</em></sub>)</span>. These estimates are obtained by maximizing the expectation of the complete maximum likelihood <span class="math inline"><em>Q</em>(<em>Θ</em>, <em>Θ</em><sup><em>q</em></sup>)</span> : <br /><span class="math display">$$Q\left(\Theta, \Theta^{q}\right)=\sum_i^N \sum_k^K t_{ik}^{q}\log \left(p_{k} \frac{1}{(2 \pi)^{p / 2}\left|\Sigma_{k}\right|^{1 / 2}} \exp \left\{-\frac{1}{2}\left(\mathbf{x_i}-\mu_{k}\right)^{t} \Sigma_{k}^{-1}\left(\mathbf{x_i}-\mu_{k}\right)\right\}\right)$$</span><br /> Subject to the constraint <span class="math inline">$\sum_k^K p_k=1$</span>. By using the Lagrangian multipliers, we can show that, the derived estimates to compute in the M-Step are equal to: <br /><span class="math display">$$\begin{array}{l}
\hat{p}_{k}=\frac{n_{k}^{q}}{n} \text { with } n_{k}=\sum_{i=1}^{n} t_{ik}^{q} \\
\hat{\mu}_{k}=\frac{1}{n_{k}^{q}} \sum_{i=1}^{n} t_{ik}^{q} \mathbf{x}_{i} \\
\hat{\Sigma}_{k}=\frac{1}{n_{k}^{q}} \sum_{i=1}^{n} t_{ik}^{q}\left(\mathbf{x}_{i}-\mu_{k}\right)^{t}\left(\mathbf{x}_{i}-\mu_{k}\right)
\end{array}$$</span><br /></p></li>
</ul>
<h2 id="three-different-gaussian-models-are-implemented-in-mixvarclust">Three different gaussian models are implemented in mixVarClust</h2>
<p>Fourteen different models can be derived from the GMM by doing an eigenvalue decomposition of <span class="math inline"><em>Σ</em><sub><em>k</em></sub></span>, and allowing free proportions (<span class="math inline"><em>p</em><sub><em>k</em></sub></span>), or restricting to equal proportions. The decomposition is :</p>
<p><br /><span class="math display"><em>Σ</em><sub><em>k</em></sub> = <em>λ</em><sub><em>k</em></sub><em>D</em><sub><em>k</em></sub><em>A</em><sub><em>k</em></sub><em>D</em><sub><em>k</em></sub><sup><em>t</em></sup>,</span><br /></p>
<p>where : <span class="math inline">$\lambda_k = \mid \Sigma_{k} \mid ^{\frac{1}{2}}$</span>; <span class="math inline"><em>D</em><sub><em>k</em></sub></span> is the matrix of eigenvectors of <span class="math inline"><em>Σ</em><sub><em>k</em></sub></span>. <span class="math inline"><em>A</em><sub><em>k</em></sub></span> is a diagonal matrix, containing the normalized eigenvalues of <span class="math inline"><em>Σ</em><sub><em>k</em></sub></span> in a decreasing order.</p>
<p>Theses models are categorized in 3 families. The first one is called the <em>general family</em>, it includes 8 models. The second family, more parsimonious than the first, is called the <em>diagonal family</em>, it includes 4 models. The last family, even more parsimonious, is called the <em>spherical family</em>. For more details see <span class="citation" data-cites="rmixmod">(Biernacki et al. 2008)</span>; the models presented here are mostly from the 2016 statistical documentation of Rmixmod.</p>
<p>In <em>mixVarClust</em>, I implemented 3 out of the fourteen models <em>with free proportions</em>, including 1 model in each family. The computations of the variance covariance matrices of the implemented models in the M-Step, are described below:</p>
<ul>
<li><p><strong>The general family</strong> model considers the full variance covariance matrix. The model computes the variances and the covariances of all the features. The variance covariance matrix is defined as : <br /><span class="math display">$$\Sigma_{k}=\frac{1}{n_{k}} W_k,$$</span><br /> where : <br /><span class="math display">$$W_k = t_{k}(x_i|\Theta)\sum_{i=1}^{n}\left(\mathbf{x}_{i}-\mu_{k}\right)^{t}\left(\mathbf{x}_{i}-\mu_{k} \right)$$</span><br /></p></li>
<li><p><strong>The diagonal family</strong> model is more parsimonious, it assumes that the covariances of all the features are equal to 0. The variance covariance matrix is defined as: <br /><span class="math display"><em>Σ</em><sub><em>k</em></sub> = <em>λ</em><sub><em>k</em></sub><em>B</em><sub><em>k</em></sub></span><br /> where: <br /><span class="math display">$$B_{k}=\frac{\operatorname{diag}\left(W_{k}\right)}{\left|\operatorname{diag}\left(W_{k}\right)\right|^{\frac{1}{d}}}, \quad \lambda_{k}=\frac{\left|\operatorname{diag}\left(W_{k}\right)\right|^{\frac{1}{d}}}{n_{k}}$$</span><br /></p></li>
<li><p><strong>The spherical family</strong> is extremely parsimonious, it assumes same variance and null covariance for all the features in each group. The variance-covariance matrix is : <br /><span class="math display"><em>Σ</em><sub><em>k</em></sub> = <em>λ</em><sub><em>k</em></sub><em>I</em></span><br /> and, <br /><span class="math display">$$\lambda_{k}=\frac{\operatorname{tr}\left(W_{k}\right)}{d n_{k}}$$</span><br /></p></li>
</ul>
<h1 id="the-multinomial-mixture-model">The multinomial mixture model</h1>
<h2 id="the-observations-to-be-clustered">The observations to be clustered</h2>
<p>The data is composed by only categorical variables <span class="math inline"><em>X</em> = {<em>x</em><sub>1</sub>, ...<em>x</em><sub><em>j</em></sub>..., <em>x</em><sub><em>d</em></sub>}</span> where <span class="math inline"><em>d</em></span> is the number of columns and <span class="math inline"><em>x</em><sub><em>j</em></sub> = {<em>x</em><sub><em>j</em>1</sub>, ...<em>x</em><sub><em>j</em><em>n</em></sub>}</span>, contains <span class="math inline"><em>n</em></span> observations (rows). Each categorical variable <span class="math inline"><em>x</em><sub><em>j</em></sub></span> has <span class="math inline"><em>m</em><sub><em>j</em></sub></span> number of categories, so there is <span class="math inline"><em>m</em><sub>1</sub>, ..., <em>m</em><sub><em>d</em></sub></span> categories for <span class="math inline"><em>d</em></span> variables. The total number of categories in the dataset is <span class="math inline">$m = \sum_{j=1}^dm_j$</span>.</p>
<h3 id="binary-representation-of-the-data" class="unnumbered unnumbered">Binary representation of the data</h3>
<p>The categorical data is represented by <span class="math inline"><em>n</em></span> binary vectors <span class="math inline"><em>x</em><sub><em>i</em></sub> = (<em>x</em><sub><em>i</em></sub><sup><em>j</em><em>h</em></sup>; <em>j</em> = 1, ..., <em>d</em>; <em>h</em> = 1, ..., <em>m</em><sub><em>j</em></sub>)</span>. <span class="math inline"><em>x</em><sub><em>i</em></sub><sup><em>j</em><em>h</em></sup> = 1</span> if the observation <span class="math inline"><em>i</em></span> is of the category <span class="math inline"><em>h</em></span> in the feature <span class="math inline"><em>j</em></span>. A one hot encoding is performed for each feature to obtain a new dataset with <span class="math inline"><em>n</em></span> rows and <span class="math inline"><em>m</em><sub><em>j</em></sub></span> columns, filled by 0 and 1.</p>
<h3 id="the-model" class="unnumbered unnumbered">The model</h3>
<p>The multinomial mixture model assumes that the distribution of each observation <span class="math inline"><em>x</em><sub><em>i</em></sub></span> is a mixture of multinomial distributions. For computational reasons, we usually make the hypothesis that the categorical variables are independent conditionally to the latent classes. The marginal distribution of <span class="math inline"><em>x</em><sub><em>i</em></sub></span> is as follows : <br /><span class="math display">$$f_X(x_i) = \sum_{k=1}^{K}p_kf_k(x_i| \alpha_{k}^{jh})$$</span><br /> and the multinomial distribution of <span class="math inline"><em>x</em><sub><em>i</em></sub></span> in each component of the mixture is defined as :</p>
<p><br /><span class="math display">$$f_k(x_i| \alpha_{k}^{jh})= \prod_{j=1}^{d} \prod_{h=1}^{m_{j}}\left(\alpha_{k}^{j h}\right)^{x_{i}^{jh}}
\label{multdist}$$</span><br /></p>
<p>With <span class="math inline"><em>α</em><sub><em>k</em></sub> = (<em>α</em><sub><em>k</em></sub><sup><em>j</em><em>h</em></sup>; <em>j</em> = 1, ..., <em>d</em>; <em>h</em> = 1, ..., <em>m</em><sub><em>j</em></sub>)</span>, <span class="math inline"><em>α</em><sub><em>k</em></sub><sup><em>j</em><em>h</em></sup></span> is the probability of having the category <em>h</em> in the column <span class="math inline"><em>j</em></span>, in the group <span class="math inline"><em>k</em></span>.</p>
<h2 id="the-em-algorithm-for-multinomial-mixture-model-mmm">The EM algorithm for Multinomial Mixture Model (MMM)</h2>
<p>Initializes arbitrarily the estimates of the parameters <span class="math inline"><em>Θ</em><sub><em>k</em></sub><sup>0</sup> = (<em>p̂</em><sub><em>k</em></sub>, <em>α̂</em><sub><em>k</em></sub><sup><em>j</em><em>h</em></sup>)</span>, then alternates the E-Step and M-Step until convergence.</p>
<ul>
<li><p><strong>E-Step.</strong> Computes the Maximum A Posteriori <span class="math inline"><em>t</em><sub><em>k</em></sub><sup><em>q</em></sup>(<strong>x</strong><sub><em>i</em></sub>|<em>Θ</em><sub><em>k</em></sub><sup><em>q</em></sup>)</span> with <span class="math inline"><em>θ</em><sup>0</sup></span> at the first iteration, uses <span class="math inline"><em>θ</em><sup><em>q</em></sup></span> for the next iterations. <br /><span class="math display">$$t_{k}^{q}\left(\mathbf{x}_{i} \mid \Theta^{q}\right) = \frac{\hat{p}_k f_k(\mathbf{x_i}|\hat{\alpha}_{k}^{jh}) }{\sum_{\ell=1}^{K}\hat{p}_\ell f_\ell(\mathbf{x_i}|\hat{\alpha}_{\ell}^{jh}) }$$</span><br /></p></li>
<li><p><strong>M-Step.</strong> Derives the estimates <span class="math inline"><em>α̂</em><sub><em>k</em></sub><sup><em>j</em><em>h</em></sup></span> by maximizing the expected log-likelihood :</p>
<p><br /><span class="math display">$$Q\left(\Theta, \Theta^{q}\right)=\sum_i^n \sum_k^K t_{k}^{q}\left(\mathbf{x}_{i}\right)\log \left(p_{k} \prod_{j=1}^{d} \prod_{h=1}^{m_{j}}\left(\alpha_{k}^{j h}\right)^{x_{i}^{jh}}\right),$$</span><br /> subject to the constraints <span class="math inline">$\sum_k^Kp_k=1$</span> and <span class="math inline">∑<sub><em>h</em></sub><em>α</em><sub><em>k</em></sub><sup><em>j</em><em>h</em></sup> = 1</span>. Using the Lagrangian we find the following estimates: <br /><span class="math display">$$\hat{\alpha}_{k}^{jh} = \frac{1}{n} \sum_{i = 1}^{n} t_k^q(\mathbf{x_{i}}|\Theta^q) \mathbf{x_{i}^{jh}},$$</span><br /></p></li>
</ul>
<p>In <span class="citation" data-cites="rmixmod">(Biernacki et al. 2008)</span>, the authors show that five different models can be derived. In <code>mixVarClust</code>, only one model is implemented, by computing the estimates <span class="math inline"><em>α̂</em><sub><em>k</em></sub></span> as described in the M-Step above.</p>
<h1 id="the-gaussian-multinomial-mixture-model">The Gaussian-Multinomial Mixture Model</h1>
<p>In this model we consider a dataset <span class="math inline"><em>X</em></span> composed by <span class="math inline"><em>q</em></span> quantitative variables and <span class="math inline"><em>c</em></span> categorical variables, <span class="math inline"><em>X</em> = {<em>x</em><sub>1</sub>, ...<em>x</em><sub><em>q</em></sub>; <em>x</em><sub>1</sub>, ...<em>x</em><sub><em>c</em></sub>}</span>. This model assumes that :</p>
<ul>
<li><p>The quantitative and categorical variables are independent conditionally to latent classes.</p></li>
<li><p>The categorical variables are independent conditionally to the latent classes and distributed according to the Multinomial distribution.</p></li>
<li><p>The continuous variables are distributed according to the Gaussian distribution.</p></li>
</ul>
<p>The marginal distribution of the mixture is formulated as : <br /><span class="math display">$$f_X(x_i|\Theta) = \sum_{k=1}^{K}p_kf_k(x_i| \alpha_{k}^{jh}, \mu_k, \Sigma_k),$$</span><br /> where <br /><span class="math display">$$f_k(x_i) = \prod_{j=1}^{d} \prod_{h=1}^{m_{j}}\left(\alpha_{k}^{j h}\right)^{x_{i}^{jh}}\times \frac{1}{(2 \pi)^{p / 2}\left|\Sigma_{k}\right|^{1 / 2}}\exp\Big(-\frac{1}{2}\left(\mathbf{x_i}-\mu_{k}\right)^{t}\Sigma_{k}^{-1}\left(\mathbf{x_i}-\mu_{k}\right)\Big).$$</span><br /> And the set of parameters are <span class="math inline"><em>Θ</em> = (<em>p</em><sub><em>k</em></sub>, <em>α</em><sub><em>k</em></sub><sup><em>j</em><em>h</em></sup>, <em>μ</em><sub><em>k</em></sub>, <em>Σ</em><sub><em>k</em></sub>)</span>.</p>
<h2 id="em-algorithm-for-gaussian-multinomial-mixture-model">EM algorithm for Gaussian-Multinomial Mixture Model</h2>
<p>Initializes arbitrarily the values of the parameters, <span class="math inline"><em>Θ</em><sub><em>k</em></sub><sup>0</sup> = (<em>p̂</em><sub><em>k</em></sub>, <em>α̂</em><sub><em>k</em></sub><sup><em>j</em><em>h</em></sup>, <em>μ̂</em><sub><em>k</em></sub>, <em>Σ̂</em><sub><em>k</em></sub>)</span>, then alternates the E-Step and M-Step until convergence of the incomplete likelihood.</p>
<ul>
<li><p><strong>E-Step.</strong> We compute the MAP <span class="math inline"><em>t</em><sub><em>k</em></sub><sup><em>q</em></sup>(<strong>x</strong><sub><em>i</em></sub>∣<em>Θ</em><sup><em>q</em></sup>)</span>, using <span class="math inline"><em>Θ</em><sup>0</sup></span> in the first iteration of the algorithm and <span class="math inline"><em>Θ</em><sup><em>q</em></sup></span> for the next iterations.</p>
<p><br /><span class="math display">$$t_{k}^{q}\left(\mathbf{x}_{i} \mid \Theta^{q}\right) = \frac{\hat{p}_k f_k(\mathbf{x_i}|\hat{\alpha}_{k}^{jh}, \hat{\mu}_{k}, \hat{\Sigma}_{k})}{\sum_{\ell=1}^{K}\hat{p}_\ell f_\ell(\mathbf{x_i}|\hat{\alpha}_{\ell}^{jh},\hat{\mu}_\ell, \hat{\Sigma}_\ell) }$$</span><br /></p></li>
<li><p><strong>M-Step.</strong> Maximizes <span class="math inline"><em>Q</em>(<em>Θ</em>, <em>Θ</em><sup><em>q</em></sup>)</span> with respect to the constraints <span class="math inline">$\sum_k^Kp_k = 1 \text{ and } \sum_h^{m_j}\alpha_k^{jh} = 1$</span>, then computes the derived estimates.</p>
<p><br /><span class="math display">$$Q\left(\Theta, \Theta^{q}\right)=\sum_i^N \sum_k^K t_{k}^{q}\left(\mathbf{x}_{i}\right) \log \left[p_{k} \times \prod_{j=1}^{d} \prod_{h=1}^{m_{j}}\left(\alpha_{k}^{j h}\right)^{x_{i}^{jh}}\times \frac{1}{(2 \pi)^{p / 2}\left|\Sigma_{k}\right|^{1 / 2}} \exp\Big(-\frac{1}{2}\left(\mathbf{x_i}-\mu_{k}\right)^{t}\Sigma_{k}^{-1}\left(\mathbf{x_i}-\mu_{k}\right)\Big)
\right]$$</span><br /> We can show that the derivation of <span class="math inline"><em>Q</em></span> with respect to the parameters gives the following estimates:</p>
<p><br /><span class="math display">$$\begin{array}{l}
\hat{p}_{k}=\frac{n_{k}^{q}}{n} \text { with } n_{k}=\sum_{i=1}^{n} t_{k}^{q}\left(\mathbf{x}_{i}\right) \\
\hat{\mu}_{k}=\frac{1}{n_{k}^{q}} \sum_{i=1}^{n} t_{k}^{q}\left(\mathbf{x}_{i}\right) \mathbf{x}_{i} \\
\hat{\Sigma}_{k}=\frac{1}{n_{k}^{q}} \sum_{i=1}^{n} t_{k}^{q}\left(\mathbf{x}_{i}\right)\left(\mathbf{x}_{i}-\mu_{k}\right)^{t}\left(\mathbf{x}_{i}-\mu_{k}\right)\\
\hat{\alpha}_{k}^{jh} = \frac{1}{N} \sum_{i = 1}^{N} t_k^q(\mathbf{x_{i}}|\Theta^q) \mathbf{x_{i}^{jh}}
\end{array}$$</span><br /></p></li>
</ul>
<p>The general, diagonal, and spherical families implemented for the GMM, are also implemented for the continuous part, of the mix feature model.</p>
<p><strong>Note:</strong> For model selection, there is the Bayesian information criterion (BIC) implemented in mixVarClust.</p>

<h1 id="ref">References</h1>
<div id="refs" class="references">
<div id="ref-rmixmod">
<p>Biernacki, Christophe, Gilles Celeux, Gérard Govaert, and Florent Langrognet. 2008. “MIXMOD : a software for model-based classification with continuous and categorical data.” <a href="https://hal.archives-ouvertes.fr/hal-00469522" class="uri">https://hal.archives-ouvertes.fr/hal-00469522</a>.</p>
</div>
<div id="ref-bouveyron_model-based_2019">
<p>Bouveyron, Charles, Gilles Celeux, T. Brendan Murphy, and Adrian E. Raftery, eds. 2019. <em>Model-Based Clustering and Classification for Data Science: With Applications in R</em>. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge ; New York, NY: Cambridge University Press.</p>
</div>
<div id="ref-celeux_classification_1992">
<p>Celeux, Gilles, and Gérard Govaert. 1992. “A Classification EM Algorithm for Clustering and Two Stochastic Versions.” <em>Computational Statistics &amp; Data Analysis</em> 14 (3): 315–32. <a href="https://doi.org/10.1016/0167-9473(92)90042-E" class="uri">https://doi.org/10.1016/0167-9473(92)90042-E</a>.</p>
</div>
</div>
</body>
</html>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="../../../../index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="../../../../images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'G-P6FER778X2', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>

